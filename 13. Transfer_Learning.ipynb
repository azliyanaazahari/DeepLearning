{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"MEBwvfDEbMrb"},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"2pW1hKzCbMri"},"source":["## Pytorch Transfer Learning\n","\n","We've built a few models by hand so far.\n","\n","But their performance has been poor.\n","\n","You might be thinking, is there a well-performing model that already exists for our problem?\n","\n","And in the world of deep learning, the answer is often yes.\n","\n","We'll see how by using a powerful technique called transfer learning.\n","\n","In this notebook, you will learn how to train a convolutional neural network for\n","image classification using transfer learning. You can read more about the transfer\n","learning at `cs231n notes <https://cs231n.github.io/transfer-learning/>`__\n","\n","Quoting these notes,\n","\n","    In practice, very few people train an entire Convolutional Network\n","    from scratch (with random initialization), because it is relatively\n","    rare to have a dataset of sufficient size. Instead, it is common to\n","    pretrain a ConvNet on a very large dataset (e.g. ImageNet, which\n","    contains 1.2 million images with 1000 categories), and then use the\n","    ConvNet either as an initialization or a fixed feature extractor for\n","    the task of interest.\n","\n","\n","## What is transfer learning?\n","\n","**Transfer learning** allows us to take the patterns (also called weights) another model has learned from another problem and use them for our own problem.\n","\n","For example, we can take the patterns a computer vision model has learned from datasets such as [ImageNet](https://www.image-net.org/) (millions of images of different objects) and use them to train new model on CIFAR10.\n","\n","Or we could take the patterns from a [language model](https://developers.google.com/machine-learning/glossary#masked-language-model) (a model that's been through large amounts of text to learn a representation of language) and use them as the basis of a model to classify different text samples.\n","\n","The premise remains: find a well-performing existing model and apply it to your own problem.\n","\n","<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-transfer-learning-example-overview.png\" alt=\"transfer learning overview on different problems\" width=900/>\n","\n","*Example of transfer learning being applied to computer vision and natural language processing (NLP). In the case of computer vision, a computer vision model might learn patterns on millions of images in ImageNet and then use those patterns to infer on another problem. And for NLP, a language model may learn the structure of language by reading all of Wikipedia (and perhaps more) and then apply that knowledge to a different problem.*\n","\n","\n","## Where to find pretrained models\n","\n","The world of deep learning is an amazing place.\n","\n","So amazing that many people around the world share their work.\n","\n","Often, code and pretrained models for the latest state-of-the-art research is released within a few days of publishing.\n","\n","And there are several places you can find pretrained models to use for your own problems.\n","\n","| **Location** | **What's there?** | **Link(s)** |\n","| ----- | ----- | ----- |\n","| **PyTorch domain libraries** | Each of the PyTorch domain libraries (`torchvision`, `torchtext`) come with pretrained models of some form. The models there work right within PyTorch. | [`torchvision.models`](https://pytorch.org/vision/stable/models.html), [`torchtext.models`](https://pytorch.org/text/main/models.html), [`torchaudio.models`](https://pytorch.org/audio/stable/models.html), [`torchrec.models`](https://pytorch.org/torchrec/torchrec.models.html) |\n","| **HuggingFace Hub** | A series of pretrained models on many different domains (vision, text, audio and more) from organizations around the world. There's plenty of different datasets too. | https://huggingface.co/models, https://huggingface.co/datasets |\n","| **`timm` (PyTorch Image Models) library** | Almost all of the latest and greatest computer vision models in PyTorch code as well as plenty of other helpful computer vision features. | https://github.com/rwightman/pytorch-image-models|\n","| **Paperswithcode** | A collection of the latest state-of-the-art machine learning papers with code implementations attached. You can also find benchmarks here of model performance on different tasks. | https://paperswithcode.com/ |\n","\n","<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-transfer-learning-where-to-find-pretrained-models.png\" alt=\"different locations to find pretrained neural network models\" width=900/>\n","\n","*With access to such high-quality resources as above, it should be common practice at the start of every deep learning problem you take on to ask, \"Does a pretrained model exist for my problem?\"*\n","\n","> **Exercise:** Spend 5-minutes going through [`torchvision.models`](https://pytorch.org/vision/stable/models.html) as well as the [HuggingFace Hub Models page](https://huggingface.co/models), what do you find? (there's no right answers here, it's just to practice exploring)\n","\n","\n","These two major transfer learning scenarios look as follows:\n","\n","-  **Finetuning the convnet**: Instead of random initialization, we\n","   initialize the network with a pretrained network, like the one that is\n","   trained on imagenet 1000 dataset. Rest of the training looks as\n","   usual.\n","-  **ConvNet as fixed feature extractor**: Here, we will freeze the weights\n","   for all of the network except that of the final fully connected\n","   layer. This last fully connected layer is replaced with a new one\n","   with random weights and only this layer is trained.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"V1CkAbw0bMrn","executionInfo":{"status":"ok","timestamp":1694661301008,"user_tz":-480,"elapsed":9795,"user":{"displayName":"Hasan Firdaus","userId":"10430079172461636773"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"95ae88c8-2c31-4f8e-d874-953f21117a4e"},"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Couldn't find torchinfo... installing it.\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","\n","# Try to get torchinfo, install it if it doesn't work\n","try:\n","    from torchinfo import summary\n","except:\n","    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n","    !pip install -q torchinfo\n","    from torchinfo import summary\n","\n","# move the model to GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{"id":"XDLpHs_ZbMrp"},"source":["## 1. Load Data\n","---------\n","\n","Before we can start to use **transfer learning**, we'll need a dataset.\n","\n","To see how transfer learning compares to our previous attempts at model building, we'll use the same CIFAR10 for experimentation.\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"dD325sGgbMrq","outputId":"dca09454-ec6b-4b01-a41e-a7f16f26a4db","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694661450106,"user_tz":-480,"elapsed":8551,"user":{"displayName":"Hasan Firdaus","userId":"10430079172461636773"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:03<00:00, 45990146.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/cifar-10-python.tar.gz to data\n","Files already downloaded and verified\n"]}],"source":["# Define transformations\n","transform = transforms.Compose(\n","    [transforms.Resize(224), # Note the difference here compared to before\n","     transforms.ToTensor(),\n","     transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n","\n","# Setup training data\n","train_data = datasets.CIFAR10(\n","    root=\"data\", # where to download data to?\n","    train=True, # get training data\n","    download=True, # download data if it doesn't exist on disk\n","    transform=transform, # images come as PIL format, we want to turn into Torch tensors and normalize\n","    target_transform=None # you can transform labels as well\n",")\n","\n","# Setup testing data\n","test_data = datasets.CIFAR10(\n","    root=\"data\",\n","    train=False, # get test data\n","    download=True,\n","    transform=transform\n",")"]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","# Setup the batch size hyperparameter\n","BATCH_SIZE = 4\n","\n","# Turn datasets into iterables (batches)\n","train_dataloader = DataLoader(train_data, # dataset to turn into iterable\n","    batch_size=BATCH_SIZE, # how many samples per batch?\n","    shuffle=True # shuffle data every epoch?\n",")\n","\n","test_dataloader = DataLoader(test_data,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False # don't necessarily have to shuffle the testing data\n",")\n","\n","# Let's check out what we've created\n","print(f\"Dataloaders: {train_dataloader, test_dataloader}\")\n","print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n","print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GI-KAx65shYB","executionInfo":{"status":"ok","timestamp":1694661455828,"user_tz":-480,"elapsed":340,"user":{"displayName":"Hasan Firdaus","userId":"10430079172461636773"}},"outputId":"a40cf27a-4636-4b4f-be61-c6434588addb"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x79b2007c1db0>, <torch.utils.data.dataloader.DataLoader object at 0x79b2007c0280>)\n","Length of train dataloader: 12500 batches of 4\n","Length of test dataloader: 2500 batches of 4\n"]}]},{"cell_type":"markdown","metadata":{"id":"fzBJhridbMrr"},"source":["## 2. Define the training loop\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"16shNGOgbMrs","executionInfo":{"status":"ok","timestamp":1694661466140,"user_tz":-480,"elapsed":504,"user":{"displayName":"Hasan Firdaus","userId":"10430079172461636773"}}},"outputs":[],"source":["import time\n","from tqdm.auto import tqdm\n","\n","def train_and_validate(model, loss_criterion, optimizer, train_dataloader, test_dataloader, epochs=25, device='cuda'):\n","    '''\n","    Function to train and validate\n","    Parameters\n","        :param model: Model to train and validate\n","        :param loss_criterion: Loss Criterion to minimize\n","        :param optimizer: Optimizer for computing gradients\n","        :param train_dataloader: DataLoader for training data\n","        :param test_dataloader: DataLoader for test/validation data\n","        :param epochs: Number of epochs (default=25)\n","        :param device: Device to perform computations ('cuda' or 'cpu')\n","\n","    Returns\n","        model: Trained Model with best validation accuracy\n","        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n","    '''\n","\n","    start = time.time()\n","    history = []\n","    best_acc = 0.0\n","\n","    # accuracy = torchmetrics.Accuracy(device=device)\n","    # Initialize the accuracy metric from torchmetrics\n","    # accuracy = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=10).to(device)\n","\n","    for epoch in tqdm(range(epochs)):\n","        epoch_start = time.time()\n","        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n","\n","        model.train()\n","\n","        train_loss = 0.0\n","        train_acc = 0.0\n","\n","        valid_loss = 0.0\n","        valid_acc = 0.0\n","\n","        for i, (inputs, labels) in enumerate(train_dataloader):\n","\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            # Clean existing gradients\n","            optimizer.zero_grad()\n","\n","            # Forward pass - compute outputs on input data using the model\n","            outputs = model(inputs)\n","\n","            # Compute loss\n","            loss = loss_criterion(outputs, labels)\n","\n","            # Backpropagate the gradients\n","            loss.backward()\n","\n","            # Update the parameters\n","            optimizer.step()\n","\n","            # Compute the total loss for the batch and add it to train_loss\n","            train_loss += loss.item() * inputs.size(0)\n","\n","            # Compute the accuracy\n","            ret, predictions = torch.max(outputs.data, 1)\n","            correct_counts = predictions.eq(labels.data.view_as(predictions))\n","\n","            # Convert correct_counts to float and then compute the mean\n","            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n","\n","            # Compute total accuracy in the whole batch and add to train_acc\n","            train_acc += acc.item() * inputs.size(0)\n","\n","        # Validation - No gradient tracking needed\n","        with torch.no_grad():\n","\n","            model.eval()\n","\n","            # Validation loop\n","            for j, (inputs, labels) in enumerate(test_dataloader):\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # Forward pass - compute outputs on input data using the model\n","                outputs = model(inputs)\n","\n","                # Compute loss\n","                loss = loss_criterion(outputs, labels)\n","\n","                # Compute the total loss for the batch and add it to valid_loss\n","                valid_loss += loss.item() * inputs.size(0)\n","\n","                # Calculate validation accuracy\n","                ret, predictions = torch.max(outputs.data, 1)\n","                correct_counts = predictions.eq(labels.data.view_as(predictions))\n","\n","                # Convert correct_counts to float and then compute the mean\n","                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n","\n","                # Compute total accuracy in the whole batch and add to valid_acc\n","                valid_acc += acc.item() * inputs.size(0)\n","\n","\n","        # Find average training loss and training accuracy\n","        avg_train_loss = train_loss / len(train_dataloader.dataset)\n","        avg_train_acc = train_acc / len(train_dataloader.dataset)\n","\n","        # Find average validation loss and training accuracy\n","        avg_test_loss = valid_loss / len(test_dataloader.dataset)\n","        avg_test_acc = valid_acc / len(test_dataloader.dataset)\n","\n","        history.append([avg_train_loss, avg_test_loss, avg_train_acc, avg_test_acc])\n","\n","        epoch_end = time.time()\n","\n","        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc * 100, avg_test_loss, avg_test_acc * 100, epoch_end - epoch_start))\n","\n","        # Save if the model has best accuracy till now\n","        if avg_test_acc > best_acc:\n","            best_acc = avg_test_acc\n","            best_model = model\n","            torch.save(best_model, 'best_model.pt')\n","\n","    return best_model, history"]},{"cell_type":"markdown","metadata":{"id":"7sWYeYbAbMrw"},"source":["## 3. Finetuning the convnet\n","----------------------\n","\n","Load a pretrained model and define a **NEW** final fully connected layer.\n","\n","Since we're working on a computer vision problem, we can find pretrained classification models in [`torchvision.models`](https://pytorch.org/vision/stable/models.html#classification).\n","\n","Exploring the documentation, you'll find plenty of common computer vision architecture backbones such as:\n","\n","| **Architecuture backbone** | **Code** |\n","| ----- | ----- |\n","| [ResNet](https://arxiv.org/abs/1512.03385)'s | `torchvision.models.resnet18()`, `torchvision.models.resnet50()`... |\n","| [VGG](https://arxiv.org/abs/1409.1556) (similar to what we used for TinyVGG) | `torchvision.models.vgg16()` |\n","| [EfficientNet](https://arxiv.org/abs/1905.11946)'s | `torchvision.models.efficientnet_b0()`, `torchvision.models.efficientnet_b1()`... |\n","| [VisionTransformer](https://arxiv.org/abs/2010.11929) (ViT's)| `torchvision.models.vit_b_16()`, `torchvision.models.vit_b_32()`... |\n","| [ConvNeXt](https://arxiv.org/abs/2201.03545) | `torchvision.models.convnext_tiny()`,  `torchvision.models.convnext_small()`... |\n","| More available in `torchvision.models` | `torchvision.models...` |\n","\n","### 3.1 Which pretrained model should you use?\n","\n","It depends on your problem/the device you're working with.\n","\n","Generally, the higher number in the model name (e.g. `resnet18()` -> `resnet50()` -> `resnet152()`) means *better performance* but a *larger* model.\n","\n","You might think better performance is *always better*, right?\n","\n","That's true but **some better performing models are too big for some devices**.\n","\n","For example, say you'd like to run your model on a mobile-device, you'll have to take into account the limited compute resources on the device, thus you'd be looking for a smaller model.\n","\n","But if you've got unlimited compute power, you'd likely take the biggest, most compute hungry model you can.\n","\n","Understanding this **performance vs. speed vs. size tradeoff** will come with time and practice.\n","\n","For example, a nice balance can be found in the `efficientnet_bX` models.\n","\n","As of May 2022, [Nutrify](https://nutrify.app) (the machine learning powered app I'm working on) is powered by an `efficientnet_b0`.\n","\n","[Comma.ai](https://comma.ai/) (a company that makes open source self-driving car software) [uses an `efficientnet_b2`](https://geohot.github.io/blog/jekyll/update/2021/10/29/an-architecture-for-life.html) to learn a representation of the road."]},{"cell_type":"markdown","source":["### 3.2 Setting up a pretrained model\n","\n","The pretrained model we're going to be using is [`torchvision.models.resnet18()`](https://pytorch.org/vision/master/models/generated/torchvision.models.resnet18.html#torchvision.models.resnet18).\n","\n","The architecture is from the paper *[Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)*.\n","\n","<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-effnet-b0-feature-extractor.png\" alt=\"efficienet_b0 from PyTorch torchvision feature extraction model\" width=900/>\n","\n","*Example of what we're going to create, a pretrained resnet18 from `torchvision.models` with the output layer adjusted for our use case of classifying objects in CIFAR10."],"metadata":{"id":"tPuFhkHQlcm-"}},{"cell_type":"markdown","source":["Let say we are using the Resnet 18 as our pretrained model. We can download and visualize the layers:"],"metadata":{"id":"lnfAH-_auHBc"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"0SyIHmRpbMrx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694661474500,"user_tz":-480,"elapsed":894,"user":{"displayName":"Hasan Firdaus","userId":"10430079172461636773"}},"outputId":"2e3b7773-934a-4aee-8a40-671a1c453249"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 263MB/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",")"]},"metadata":{},"execution_count":6}],"source":["model_ft = models.resnet18(pretrained=True)\n","model_ft"]},{"cell_type":"code","source":["summary(model=model_ft,\n","        input_size=(4, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n","        # col_names=[\"input_size\"], # uncomment for smaller output\n","        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","        col_width=20,\n","        row_settings=[\"var_names\"]\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z0aBkxgcVlIi","executionInfo":{"status":"ok","timestamp":1694661488496,"user_tz":-480,"elapsed":11548,"user":{"displayName":"Hasan Firdaus","userId":"10430079172461636773"}},"outputId":"34b01c8f-410f-49f9-9371-a7546fbe7518"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["========================================================================================================================\n","Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n","========================================================================================================================\n","ResNet (ResNet)                          [4, 3, 224, 224]     [4, 1000]            --                   True\n","├─Conv2d (conv1)                         [4, 3, 224, 224]     [4, 64, 112, 112]    9,408                True\n","├─BatchNorm2d (bn1)                      [4, 64, 112, 112]    [4, 64, 112, 112]    128                  True\n","├─ReLU (relu)                            [4, 64, 112, 112]    [4, 64, 112, 112]    --                   --\n","├─MaxPool2d (maxpool)                    [4, 64, 112, 112]    [4, 64, 56, 56]      --                   --\n","├─Sequential (layer1)                    [4, 64, 56, 56]      [4, 64, 56, 56]      --                   True\n","│    └─BasicBlock (0)                    [4, 64, 56, 56]      [4, 64, 56, 56]      --                   True\n","│    │    └─Conv2d (conv1)               [4, 64, 56, 56]      [4, 64, 56, 56]      36,864               True\n","│    │    └─BatchNorm2d (bn1)            [4, 64, 56, 56]      [4, 64, 56, 56]      128                  True\n","│    │    └─ReLU (relu)                  [4, 64, 56, 56]      [4, 64, 56, 56]      --                   --\n","│    │    └─Conv2d (conv2)               [4, 64, 56, 56]      [4, 64, 56, 56]      36,864               True\n","│    │    └─BatchNorm2d (bn2)            [4, 64, 56, 56]      [4, 64, 56, 56]      128                  True\n","│    │    └─ReLU (relu)                  [4, 64, 56, 56]      [4, 64, 56, 56]      --                   --\n","│    └─BasicBlock (1)                    [4, 64, 56, 56]      [4, 64, 56, 56]      --                   True\n","│    │    └─Conv2d (conv1)               [4, 64, 56, 56]      [4, 64, 56, 56]      36,864               True\n","│    │    └─BatchNorm2d (bn1)            [4, 64, 56, 56]      [4, 64, 56, 56]      128                  True\n","│    │    └─ReLU (relu)                  [4, 64, 56, 56]      [4, 64, 56, 56]      --                   --\n","│    │    └─Conv2d (conv2)               [4, 64, 56, 56]      [4, 64, 56, 56]      36,864               True\n","│    │    └─BatchNorm2d (bn2)            [4, 64, 56, 56]      [4, 64, 56, 56]      128                  True\n","│    │    └─ReLU (relu)                  [4, 64, 56, 56]      [4, 64, 56, 56]      --                   --\n","├─Sequential (layer2)                    [4, 64, 56, 56]      [4, 128, 28, 28]     --                   True\n","│    └─BasicBlock (0)                    [4, 64, 56, 56]      [4, 128, 28, 28]     --                   True\n","│    │    └─Conv2d (conv1)               [4, 64, 56, 56]      [4, 128, 28, 28]     73,728               True\n","│    │    └─BatchNorm2d (bn1)            [4, 128, 28, 28]     [4, 128, 28, 28]     256                  True\n","│    │    └─ReLU (relu)                  [4, 128, 28, 28]     [4, 128, 28, 28]     --                   --\n","│    │    └─Conv2d (conv2)               [4, 128, 28, 28]     [4, 128, 28, 28]     147,456              True\n","│    │    └─BatchNorm2d (bn2)            [4, 128, 28, 28]     [4, 128, 28, 28]     256                  True\n","│    │    └─Sequential (downsample)      [4, 64, 56, 56]      [4, 128, 28, 28]     8,448                True\n","│    │    └─ReLU (relu)                  [4, 128, 28, 28]     [4, 128, 28, 28]     --                   --\n","│    └─BasicBlock (1)                    [4, 128, 28, 28]     [4, 128, 28, 28]     --                   True\n","│    │    └─Conv2d (conv1)               [4, 128, 28, 28]     [4, 128, 28, 28]     147,456              True\n","│    │    └─BatchNorm2d (bn1)            [4, 128, 28, 28]     [4, 128, 28, 28]     256                  True\n","│    │    └─ReLU (relu)                  [4, 128, 28, 28]     [4, 128, 28, 28]     --                   --\n","│    │    └─Conv2d (conv2)               [4, 128, 28, 28]     [4, 128, 28, 28]     147,456              True\n","│    │    └─BatchNorm2d (bn2)            [4, 128, 28, 28]     [4, 128, 28, 28]     256                  True\n","│    │    └─ReLU (relu)                  [4, 128, 28, 28]     [4, 128, 28, 28]     --                   --\n","├─Sequential (layer3)                    [4, 128, 28, 28]     [4, 256, 14, 14]     --                   True\n","│    └─BasicBlock (0)                    [4, 128, 28, 28]     [4, 256, 14, 14]     --                   True\n","│    │    └─Conv2d (conv1)               [4, 128, 28, 28]     [4, 256, 14, 14]     294,912              True\n","│    │    └─BatchNorm2d (bn1)            [4, 256, 14, 14]     [4, 256, 14, 14]     512                  True\n","│    │    └─ReLU (relu)                  [4, 256, 14, 14]     [4, 256, 14, 14]     --                   --\n","│    │    └─Conv2d (conv2)               [4, 256, 14, 14]     [4, 256, 14, 14]     589,824              True\n","│    │    └─BatchNorm2d (bn2)            [4, 256, 14, 14]     [4, 256, 14, 14]     512                  True\n","│    │    └─Sequential (downsample)      [4, 128, 28, 28]     [4, 256, 14, 14]     33,280               True\n","│    │    └─ReLU (relu)                  [4, 256, 14, 14]     [4, 256, 14, 14]     --                   --\n","│    └─BasicBlock (1)                    [4, 256, 14, 14]     [4, 256, 14, 14]     --                   True\n","│    │    └─Conv2d (conv1)               [4, 256, 14, 14]     [4, 256, 14, 14]     589,824              True\n","│    │    └─BatchNorm2d (bn1)            [4, 256, 14, 14]     [4, 256, 14, 14]     512                  True\n","│    │    └─ReLU (relu)                  [4, 256, 14, 14]     [4, 256, 14, 14]     --                   --\n","│    │    └─Conv2d (conv2)               [4, 256, 14, 14]     [4, 256, 14, 14]     589,824              True\n","│    │    └─BatchNorm2d (bn2)            [4, 256, 14, 14]     [4, 256, 14, 14]     512                  True\n","│    │    └─ReLU (relu)                  [4, 256, 14, 14]     [4, 256, 14, 14]     --                   --\n","├─Sequential (layer4)                    [4, 256, 14, 14]     [4, 512, 7, 7]       --                   True\n","│    └─BasicBlock (0)                    [4, 256, 14, 14]     [4, 512, 7, 7]       --                   True\n","│    │    └─Conv2d (conv1)               [4, 256, 14, 14]     [4, 512, 7, 7]       1,179,648            True\n","│    │    └─BatchNorm2d (bn1)            [4, 512, 7, 7]       [4, 512, 7, 7]       1,024                True\n","│    │    └─ReLU (relu)                  [4, 512, 7, 7]       [4, 512, 7, 7]       --                   --\n","│    │    └─Conv2d (conv2)               [4, 512, 7, 7]       [4, 512, 7, 7]       2,359,296            True\n","│    │    └─BatchNorm2d (bn2)            [4, 512, 7, 7]       [4, 512, 7, 7]       1,024                True\n","│    │    └─Sequential (downsample)      [4, 256, 14, 14]     [4, 512, 7, 7]       132,096              True\n","│    │    └─ReLU (relu)                  [4, 512, 7, 7]       [4, 512, 7, 7]       --                   --\n","│    └─BasicBlock (1)                    [4, 512, 7, 7]       [4, 512, 7, 7]       --                   True\n","│    │    └─Conv2d (conv1)               [4, 512, 7, 7]       [4, 512, 7, 7]       2,359,296            True\n","│    │    └─BatchNorm2d (bn1)            [4, 512, 7, 7]       [4, 512, 7, 7]       1,024                True\n","│    │    └─ReLU (relu)                  [4, 512, 7, 7]       [4, 512, 7, 7]       --                   --\n","│    │    └─Conv2d (conv2)               [4, 512, 7, 7]       [4, 512, 7, 7]       2,359,296            True\n","│    │    └─BatchNorm2d (bn2)            [4, 512, 7, 7]       [4, 512, 7, 7]       1,024                True\n","│    │    └─ReLU (relu)                  [4, 512, 7, 7]       [4, 512, 7, 7]       --                   --\n","├─AdaptiveAvgPool2d (avgpool)            [4, 512, 7, 7]       [4, 512, 1, 1]       --                   --\n","├─Linear (fc)                            [4, 512]             [4, 1000]            513,000              True\n","========================================================================================================================\n","Total params: 11,689,512\n","Trainable params: 11,689,512\n","Non-trainable params: 0\n","Total mult-adds (G): 7.26\n","========================================================================================================================\n","Input size (MB): 2.41\n","Forward/backward pass size (MB): 158.99\n","Params size (MB): 46.76\n","Estimated Total Size (MB): 208.16\n","========================================================================================================================"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["Our main objective is to **REPLACE** the final classification layer (containing 1000 classes of Imagenet) with a **NEW** classification layer (containing 10, corresponding to 10 classes in CIFAR10)\n","\n","Therefore, we are accessing the in_features of the fc (fully connected) layer and connect it to a newly defined layer containing 10 neurons:"],"metadata":{"id":"9Xmo2Dwkuf2v"}},{"cell_type":"code","source":["num_ftrs = model_ft.fc.in_features\n","# Here the size of each output sample is set to 10.\n","# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(train_data.classes)).\n","model_ft.fc = nn.Linear(num_ftrs, 10)\n","\n","# LOSS AND OPTIMIZER\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n","\n","# move the model to GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","model_ft = model_ft.to(device)"],"metadata":{"id":"F80BdM7auS5_","executionInfo":{"status":"ok","timestamp":1694661500927,"user_tz":-480,"elapsed":337,"user":{"displayName":"Hasan Firdaus","userId":"10430079172461636773"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"THo1KYFMbMry"},"source":["### Train and evaluate\n","\n","It should take around 15-25 min on CPU. On GPU though, it takes less than a\n","minute.\n","\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"ifwbrqKqbMrz","outputId":"9d4e7bfd-8efd-4ff1-b3d6-48f77e9027e1","colab":{"base_uri":"https://localhost:8080/","height":570,"referenced_widgets":["0ed94eb6f2074e28b2351f0fac999775","7f56da4f062c49f28015e27b66d14b02","d7459975b59f431fb87b32935d367f6a","be77824e936f4dbba9b3b7bfa878d7e8","d4898256c4b648c4971df8229b1c2d04","64974047f3a14f10805a9a58fda599ba","99ccdb5088b549c5a09d5f7433dcc994","6069c773221440938cbb6e66c01d4abf","7f3fa1f5d0f84eea8c22ba38edd836e1","fb378f359f724172b423de8864d022ba","41ee5165ccf04523b561e1a7d0532b66"]},"executionInfo":{"status":"ok","timestamp":1694664942091,"user_tz":-480,"elapsed":3438832,"user":{"displayName":"Hasan Firdaus","userId":"10430079172461636773"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/10 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ed94eb6f2074e28b2351f0fac999775"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 1/10\n","Epoch : 000, Training: Loss: 0.9576, Accuracy: 69.1860%, \n","\t\tValidation : Loss : 3.0160, Accuracy: 14.2400%, Time: 335.7338s\n","Epoch: 2/10\n","Epoch : 001, Training: Loss: 0.6134, Accuracy: 80.4060%, \n","\t\tValidation : Loss : 4.0208, Accuracy: 10.0000%, Time: 343.4072s\n","Epoch: 3/10\n","Epoch : 002, Training: Loss: 0.4918, Accuracy: 84.1680%, \n","\t\tValidation : Loss : 8.5990, Accuracy: 10.0000%, Time: 343.7279s\n","Epoch: 4/10\n","Epoch : 003, Training: Loss: 0.4246, Accuracy: 86.2200%, \n","\t\tValidation : Loss : 5.0589, Accuracy: 9.4700%, Time: 343.3399s\n","Epoch: 5/10\n","Epoch : 004, Training: Loss: 0.3711, Accuracy: 88.2500%, \n","\t\tValidation : Loss : 4.5617, Accuracy: 10.2100%, Time: 346.0508s\n","Epoch: 6/10\n","Epoch : 005, Training: Loss: 0.3269, Accuracy: 89.4540%, \n","\t\tValidation : Loss : 5.8620, Accuracy: 10.0000%, Time: 343.9151s\n","Epoch: 7/10\n","Epoch : 006, Training: Loss: 0.2927, Accuracy: 90.6320%, \n","\t\tValidation : Loss : 6.5384, Accuracy: 10.0000%, Time: 344.0573s\n","Epoch: 8/10\n","Epoch : 007, Training: Loss: 0.2677, Accuracy: 91.4720%, \n","\t\tValidation : Loss : 5.0206, Accuracy: 9.6900%, Time: 345.0657s\n","Epoch: 9/10\n","Epoch : 008, Training: Loss: 0.2455, Accuracy: 91.9920%, \n","\t\tValidation : Loss : 6.3113, Accuracy: 10.0300%, Time: 346.4739s\n","Epoch: 10/10\n","Epoch : 009, Training: Loss: 0.2274, Accuracy: 92.4900%, \n","\t\tValidation : Loss : 5.3632, Accuracy: 11.8700%, Time: 346.6387s\n"]}],"source":["num_epochs = 10\n","trained_model, history = train_and_validate(model_ft, loss_fn, optimizer, train_dataloader, test_dataloader, num_epochs)"]},{"cell_type":"code","source":["#Analyze the loss curve\n","\n","class_names = train_data.classes\n","\n","def plot_loss(history):\n","  history = np.array(history)\n","  plt.plot(history[:,0:2])\n","  plt.legend(['Tr Loss', 'Val Loss'])\n","  plt.xlabel('Epoch Number')\n","  plt.ylabel('Loss')\n","  plt.ylim(0,3)\n","  # plt.savefig('cifar10_loss_curve.png')\n","  plt.show()\n","\n","def plot_accuracy(history):\n","  history = np.array(history)\n","  plt.plot(history[:,2:4])\n","  plt.legend(['Tr Accuracy', 'Val Accuracy'])\n","  plt.xlabel('Epoch Number')\n","  plt.ylabel('Accuracy')\n","  plt.ylim(0,1)\n","  # plt.savefig('cifar10_accuracy_curve.png')\n","  plt.show()\n","\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sn\n","import pandas as pd\n","\n","def plot_confusionMatrix(model, test_dataloader):\n","\n","  y_pred = []\n","  y_true = []\n","\n","  model.to('cpu')\n","\n","  # iterate over test data\n","  for inputs, labels in test_dataloader:\n","          output = model(inputs) # Feed Network\n","\n","          output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n","          y_pred.extend(output) # Save Prediction\n","\n","          labels = labels.data.cpu().numpy()\n","          y_true.extend(labels) # Save Truth\n","\n","  # Build confusion matrix\n","  cf_matrix = confusion_matrix(y_true, y_pred)\n","  df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) *10, index = [i for i in class_names],\n","                      columns = [i for i in class_names])\n","  plt.figure(figsize = (20,10))\n","  sn.heatmap(df_cm, annot=True)\n","  # plt.savefig('output.png')"],"metadata":{"id":"_urjGvYzX3E2"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mEcXbXkIbMrz"},"outputs":[],"source":["plot_loss(history)\n","plot_accuracy(history)"]},{"cell_type":"code","source":["plot_confusionMatrix(trained_model, test_dataloader)"],"metadata":{"id":"swmdaXjdY2Xa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Edk51HbqbMr0"},"source":["ConvNet as fixed feature extractor\n","----------------------------------\n","\n","Here, we need to freeze all the network except the final layer. We need\n","to set ``requires_grad = False`` to freeze the parameters so that the\n","gradients are not computed in ``backward()``.\n","\n","You can read more about this in the documentation\n","`here <https://pytorch.org/docs/notes/autograd.html#excluding-subgraphs-from-backward>`__.\n","\n","\n"]},{"cell_type":"code","source":["model_conv = torchvision.models.resnet18(pretrained=True)\n","for param in model_conv.parameters():\n","    param.requires_grad = False\n","\n","# Parameters of newly constructed modules have requires_grad=True by default\n","num_ftrs = model_conv.fc.in_features\n","model_conv.fc = nn.Linear(num_ftrs, 2)\n","\n","model_conv = model_conv.to(device)"],"metadata":{"id":"Wy2MNxvzkRfi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_ftrs = model_conv.fc.in_features\n","# Here the size of each output sample is set to 10.\n","# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(train_data.classes)).\n","model_conv.fc = nn.Linear(num_ftrs, 10)\n","\n","# LOSS AND OPTIMIZER\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model_conv.parameters(), lr=0.001, momentum=0.9)\n","\n","# move the model to GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","model_conv = model_conv.to(device)"],"metadata":{"id":"rtecEFD4kSaw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary(model=model_conv,\n","        input_size=(4, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n","        # col_names=[\"input_size\"], # uncomment for smaller output\n","        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","        col_width=20,\n","        row_settings=[\"var_names\"]\n",")"],"metadata":{"id":"BwuY5c6UmiiE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VvY-zKkKbMr1"},"source":["Train and evaluate\n","^^^^^^^^^^^^^^^^^^\n","\n","On CPU this will take about half the time compared to previous scenario.\n","This is expected as gradients don't need to be computed for most of the\n","network. However, forward does need to be computed.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cLs3EULwbMr1"},"outputs":[],"source":["num_epochs = 10\n","trained_model, history = train_and_validate(model_conv, loss_fn, optimizer, train_dataloader, test_dataloader, num_epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fn-gDyEIbMr2"},"outputs":[],"source":["plot_loss(history)\n","plot_accuracy(history)"]},{"cell_type":"code","source":["plot_confusionMatrix(trained_model, test_dataloader)"],"metadata":{"id":"sPWFWHNtk6hV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0wesCJc-bMr3"},"source":["Further Learning\n","-----------------\n","\n","If you would like to learn more about the applications of transfer learning,\n","checkout our `Quantized Transfer Learning for Computer Vision Tutorial <https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html>`_.\n","\n","\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"0ed94eb6f2074e28b2351f0fac999775":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7f56da4f062c49f28015e27b66d14b02","IPY_MODEL_d7459975b59f431fb87b32935d367f6a","IPY_MODEL_be77824e936f4dbba9b3b7bfa878d7e8"],"layout":"IPY_MODEL_d4898256c4b648c4971df8229b1c2d04"}},"7f56da4f062c49f28015e27b66d14b02":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64974047f3a14f10805a9a58fda599ba","placeholder":"​","style":"IPY_MODEL_99ccdb5088b549c5a09d5f7433dcc994","value":"100%"}},"d7459975b59f431fb87b32935d367f6a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6069c773221440938cbb6e66c01d4abf","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7f3fa1f5d0f84eea8c22ba38edd836e1","value":10}},"be77824e936f4dbba9b3b7bfa878d7e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb378f359f724172b423de8864d022ba","placeholder":"​","style":"IPY_MODEL_41ee5165ccf04523b561e1a7d0532b66","value":" 10/10 [57:18&lt;00:00, 345.48s/it]"}},"d4898256c4b648c4971df8229b1c2d04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64974047f3a14f10805a9a58fda599ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99ccdb5088b549c5a09d5f7433dcc994":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6069c773221440938cbb6e66c01d4abf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f3fa1f5d0f84eea8c22ba38edd836e1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fb378f359f724172b423de8864d022ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41ee5165ccf04523b561e1a7d0532b66":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}